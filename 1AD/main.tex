\documentclass[a4paper,11pt]{article}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage{geometry}
\geometry{margin=1in}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{float}
\usepackage{minted}
\usemintedstyle{friendly} % or "monokai", 

% Basic link styling for readability.
\hypersetup{
  colorlinks=true,
  linkcolor=blue,
  citecolor=blue,
  urlcolor=blue
}

\title{DD2434 Advanced Machine Learning\\Assignment 1AD}
\author{Reuben Gezang}
\date{December 2025}

\begin{document}

\maketitle



\section*{D-Level}
\subsection*{Theory 1.D.1}
\subsubsection*{Question 1.1.1}
We start by stating the definition of the Kullback-Leibler divergence:
\begin{equation}
KL(q(Z) \| p(Z|X)) = \mathbb{E}_{q(\boldsymbol{Z})}[\log( \frac{q(Z)}{p(Z|X)})] = \int q(Z) \log( \frac{q(Z)}{p(Z|X)}) dZ
\end{equation}
Now we separate the fraction inside the logarithm:
\begin{equation}
KL(q(Z) \| p(Z|X)) = \int q(Z) \log( \frac{q(Z)p(X)}{p(X,Z)}) dZ = \int q(Z) (\log(q(Z)) - \log(p(Z|X)) dZ
\end{equation}
Note that $p(Z|X) = \frac{p(X,Z)}{p(X)}$ and using this we can rewrite the equation as:
\begin{equation}
KL(q(Z) \| p(Z|X)) = \int q(Z) \log(q(Z)) dZ - \int q(Z) \log(p(X,Z)) dZ + \log(p(X)) \int q(Z) dZ
\end{equation}
Since $q(Z)$ is a probability distribution, we know that $\int q(Z) dZ = 1$. Now we can solve for $\log(p(X))$:
\begin{equation}
\log(p(X)) = KL(q(Z) \| p(Z|X)) - \int q(Z) \log(q(Z)) dZ + \int q(Z) \log(p(X,Z)) dZ
\end{equation}
Combining the logarithm terms, we get:
\begin{equation}
\log(p(X)) = KL(q(Z) \| p(Z|X)) + \mathbb{E}_{q(\boldsymbol{Z})}[\frac{\log(p(X,Z))}{q(Z)}]
\end{equation}
Now we can identify the Evidence lower bound (ELBO)
\begin{equation}
\mathcal{L}(q) = \mathbb{E}_{q(\boldsymbol{Z})}[\frac{\log(p(X,Z))}{q(Z)}]
\end{equation}
and with this we have shown that:
\begin{equation}
\log(p(X)) = \mathcal{L}(q) + KL(q(Z) \| p(Z|X))
\end{equation}
concluding the proof.
\subsubsection*{Question 1.1.2}
In this question we are to describe (in one sentence) how the choice of variational family $q(Z)$ affects
\begin{itemize}
  \item (i) The tightness of the ELBO
  \item (ii) The accuracy of the posterior approximation
\end{itemize}
(i) A more expressive variational family can lead to a tighter ELBO as it can better approximate (and better match) the true posterior, reducing the KL divergence term.\\
\\
(ii) The choice of variational family directly impacts the accuracy of the posterior approximation, as a limited family may not capture the true posterior's complexity, leading to a less accurate approximation.
\subsection*{Question 1.D.2}
\subsubsection*{1.1.3}
For a mean field assumption and joint distribution
\begin{equation*}
  q(Z_1, Z_2, Z_3) = q_1(Z_1)q_2(Z_2)q_3(Z_3), \quad p(X, Z)
\end{equation*}
Let $q^*_1(Z_1)$ be the $q_1$ that maximizes the ELBO. We want to show that $q_1^*$ satisfies
\begin{equation*}
  \log q_1^*(Z_1) = \mathbb{E}_{-Z_1}[\log p(X, Z)]
\end{equation*}
We can start by inspecting the ELBO:
\begin{equation*}
  \mathcal{L}(q) = \mathcal{L}(q) = \mathbb{E}_{q(\boldsymbol{Z})}[\log(\frac{p(X,Z)}{q(Z)})]=\mathbb{E}_{q}[\log p(X, Z)] - \mathbb{E}_{q}[\log q(Z)]
\end{equation*}
and using the mean field assumption we can rewrite this as:
\begin{equation*}
  \mathcal{L}(q) = \mathbb{E}_{q(Z)}[\log p(X, Z)] - \mathbb{E}_{q(Z)}[\log(q_1(Z_1)q_2(Z_2)q_3(Z_3))] 
\end{equation*}
and by separating the logarithm and taking expectations over the relevant distribution ($z_i$ is independent of $z_j$ for $i \neq j$) we get:
\begin{equation*}
  \mathbb{E}_{q(Z)}[\log(q_1(Z_1)q_2(Z_2)q_3(Z_3))] = \mathbb{E}_{q_1(Z_1)}[\log(q_1(Z_1))] + \mathbb{E}_{q_2(Z_2)}[\log(q_2(Z_2))] + \mathbb{E}_{q_3(Z_3)}[\log(q_3(Z_3))]
\end{equation*}
Since we are maximizing w.r.t $q_1(Z_1)$ we can ignore the terms that do not depend on it. Thus we can rewrite the ELBO as:
\begin{equation*}
  \mathcal{L}(q) = \mathbb{E}_{q(Z)}[\log p(X, Z)] - \mathbb{E}_{q_1(Z_1)}[\log(q_1(Z_1))] + C
\end{equation*}
where $C$ is a constant w.r.t $q_1(Z_1)$ (and can thus be ignored). Now we can rewrite the expectation over $q(Z)$ as:
\begin{equation*}
  \mathbb{E}_{q(Z)}[\log p(X, Z)] = \mathbb{E}_{q_1(Z_1)}[\mathbb{E}_{q_2(Z_2)q_3(Z_3)}[\log p(X, Z)]]
\end{equation*}
meaning that we can rewrite the ELBO as:
\begin{equation*}
  \mathcal{L}(q) = \mathbb{E}_{q_1(Z_1)}[\mathbb{E}_{q_2(Z_2)q_3(Z_3)}[\log p(X, Z)]] - \mathbb{E}_{q_1(Z_1)}[\log(q_1(Z_1))] + C
\end{equation*}
Using the fact that $\int_{Z_1} q(Z_1)dZ_1 = 1$ we will now optimize the ELBO w.r.t $q_1(Z_1)$ and with a lagrange multiplier $\lambda$.
\begin{equation}
  \frac{\partial}{\partial q_1(Z_1)} \left( \mathbb{E}_{q_1(Z_1)}[\mathbb{E}_{q_2(Z_2)q_3(Z_3)}[\log p(X, Z)]] - \mathbb{E}_{q_1(Z_1)}[\log(q_1(Z_1))] + \lambda (\int_{Z_1} q(Z_1)dZ_1 - 1) \right) = 0
\end{equation}
giving that
\begin{equation}
  \mathbb{E}_{q_2(Z_2)q_3(Z_3)}[\log p(X, Z)] - \log(q_1(Z_1)) - 1 + \lambda = 0 \rightarrow \log(q^*_1(Z_1)) = \mathbb{E}_{q_2(Z_2)q_3(Z_3)}[\log p(X, Z)] + \lambda - 1
\end{equation}
where $\lambda -1$ is a additive constant that can be ignored when normalizing $q_1^*(Z_1)$. Thus we have shown that:
\begin{equation}
  \log q_1^*(Z_1) = \mathbb{E}_{-Z_1}[\log p(X, Z)]
\end{equation}
as required.
\subsection*{Practice/Implementation - D level (I have chosen 1.D.3)}
\subsubsection*{1.2.4}
The log likelihood of the data (D) is:
\begin{equation}
  \log(P(D|\mu, \tau)) = \frac{N}{2}\log(\tau)-\frac{N}{2}\log(2\pi) - \frac{\tau}{2}\sum_{i=1}^{N}(x_i - \mu)^2
\end{equation}
The log prior for $\mu$ and $\tau$ is: (Note that $\mu|\tau \sim \mathcal{N}(\mu_0, (\lambda_0\tau)^{-1})$ and $\tau \sim \text{Gamma}(a_0, b_0)$)
\begin{equation}
  \log(P(\mu, \tau)) = \frac{1}{2}\log(\lambda_0\tau) - \frac{1}{2}\log(2\pi) - \frac{\lambda_0\tau}{2}(\mu - \mu_0)^2 + a_0\log(b_0) - \log(\Gamma(a_0)) + (a_0 - 1)\log(\tau) - b_0\tau
\end{equation}
The log-variational distribution is (Where $q(\mu) \sim \mathcal{N}(\mu_N, \lambda_N^{-1})$ and $q(\tau) \sim \text{Gamma}(a_N, b_N)$):
\begin{equation}
  \log(q(\mu, \tau)) = \frac{1}{2}\log(\lambda_N) - \frac{1}{2}\log(2\pi) - \frac{\lambda_N}{2}(\mu - \mu_N)^2 + a_N\log(b_N) - \log(\Gamma(a_N)) + (a_N - 1)\log(\tau) - b_N\tau
\end{equation}
Finally we state the score functions of the variational distributions. Let $$\omega = (\mu_N, \lambda_N, a_N, b_N)$$ be the variational parameters.
\begin{equation}
  \nabla_{\omega}\log(q(\mu, \tau)) = \begin{bmatrix}
  \lambda_N(\mu - \mu_N) \\
  \frac{1}{2\lambda_N} - \frac{1}{2}(\mu - \mu_N)^2 \\
  \psi(a_N) - \log(b_N) + \log(\tau) \\
  \frac{a_N}{b_N} - \tau
  \end{bmatrix}
\end{equation}
where $\psi$ is the digamma function. Note that we can also express the score function w.r.t $\lambda_N$ as using the variance/standard deviation.
We have that the relation between variance and precision is given by $\sigma^2 = \frac{1}{\lambda_N}$, meaning that $d\lambda = -\frac{2}{\sigma^3}d\sigma$.
Then we have that the score function w.r.t $\sigma$ (standard deviation) is given by
\begin{equation*}
  \frac{\partial \log q(\mu, \tau)}{\partial \sigma} = \frac{\partial \log q(\mu, \tau)}{\partial \lambda_N} \frac{\partial \lambda_N}{\partial \sigma} = \left( \frac{1}{2\lambda_N} - \frac{1}{2}(\mu - \mu_N)^2 \right) \left( -\frac{2}{\sigma^3} \right) = -\frac{1}{\sigma} + \frac{(\mu - \mu_N)^2}{\sigma^3}
\end{equation*}
\subsubsection*{1.2.5}
In this exercise we implement Algorithm 1 of the BBVI paper [Ranganath et al., 2014].
We reuse the data sampling script from 1.E.3 and prior parameters, meaning that
\begin{itemize}
  \item $\mu_0 = 1.0$
  \item $\lambda_0 = 0.1$
  \item $a_0 = 1.0$
  \item $b_0 = 2.0$
\end{itemize}
The following plots show the ELBO over iterations and the expected value of $\mu$ and $\tau$
over iterations for dataset 2 with 100 samples.
\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{Plots/BBVI_Normal_Fit_100_samples.png}
  \caption{ELBO over iterations for dataset with 100 samples.}
\end{figure}
\section*{C-level}
\subsection*{Theory 1.C.1}
\subsubsection*{Question 2.1.10}
For this exercise we want to show that that the IWELBO (Importance-Weighted ELBO) is a valid lower bound
on the log marginal likelihood $\log p(X)$. First of, we state the IWELBO:
\begin{equation}
  \mathcal{L}_K(q) := \mathbb{E}_{Z_{1}, \ldots, Z_{K}}\left[\ \log\left( \frac{1}{K} \sum_{k=1}^{K} \frac{p(X, Z_k)}{q(Z_k|X)}\right)\right]
\end{equation}
Now, note that we can rewrite the marginal likelihood as:
\begin{equation*}
  p(X) = \int p(X, Z) dZ = \int q(Z|X) \frac{p(X, Z)}{q(Z|X)} dZ = \mathbb{E}_{q(Z|X)}\left[\frac{p(X, Z)}{q(Z|X)}\right]
\end{equation*}
and we can extent this to $K$ samples:
\begin{equation*}
  p(X) = \mathbb{E}_{q(Z_{1}|X), \ldots, q(Z_{K}|X)}\left[\frac{1}{K} \sum_{k=1}^{K} \frac{p(X, Z_k)}{q(Z_k|X)}\right]
\end{equation*}
Now, by applying Jensen's inequality that says that for a concave function $f$ and random variable $X$, $\mathbb{E}[f(X)] \leq f(\mathbb{E}[X])$,
(note that $\log$ is concave) we get:
\begin{equation*}
  \log p(X) = \log \left(\mathbb{E}_{q(Z_{1}|X), \ldots, q(Z_{K}|X)}\left[\frac{1}{K} \sum_{k=1}^{K} \frac{p(X, Z_k)}{q(Z_k|X)}\right]\right) \geq \mathbb{E}_{q(Z_{1}|X), \ldots, q(Z_{K}|X)}\left[\log\left( \frac{1}{K} \sum_{k=1}^{K} \frac{p(X, Z_k)}{q(Z_k|X)}\right)\right]
\end{equation*}
Thus, the IWELBO is a valid lower bound on the log marginal likelihood $\log p(X)$, as required.
\subsubsection*{2.1.11}
Let $W_K = \frac{p(X,Z_K)}{q(Z_K|X)}$. The IWELBO can then be rewritten as:
\begin{equation*}
  \mathcal{L}_K(q) = \mathbb{E}_{Z_{1}, \ldots, Z_{K}}\left[\ \log\left( \frac{1}{K} \sum_{k=1}^{K} W_k\right)\right]
\end{equation*}
We can also see that the standard ELBO can be rewritten as:
\begin{equation*}
  \mathcal{L}_1(q) = \mathbb{E}_{Z_{1}}\left[\ \log\left( W_1\right)\right] = \mathbb{E}_{Z_{1}, \ldots, Z_{K}}\left[\ \frac{1}{K}\sum^K_{k=1}\log\left( W_k\right)\right]
\end{equation*}
This is because the $W_k$ are i.i.d. Now we can see that
\begin{equation*}
  \log\left(\frac{1}{K}\sum^K_{k=1} W_k \right)\geq \frac{1}{K}\sum^K_{k=1}\log\left( W_k\right)
\end{equation*}
This follows from the fact that the logarithm is strictly concave and according to Jensens inequality the logarithm of an average is greater than the average of the logaritms.
Taking expectations of both sides, we get the final expression:
\begin{equation*}
  \mathcal{L}_K(q) = \mathbb{E}_{Z_{1}, \ldots, Z_{K}}\left[\ \log\left( \frac{1}{K} \sum_{k=1}^{K} W_k\right)\right]\geq\mathbb{E}_{Z_{1}, \ldots, Z_{K}}\left[\ \frac{1}{K}\sum^K_{k=1}\log\left( W_k\right)\right] = \mathcal{L}_1(q)
\end{equation*}
\subsection*{Theory 1.C.2}
\subsubsection*{Question 2.1.12}
In this exercise we derive the Rao-Blackwellized partial gradient of the ELBO w.r.t $\lambda_3$.
We have a total of $n+4$ latent variables, $v, z, y_n, \omega_1, \omega_2$. The variational distribution is given by:
\begin{equation*}
  q(w_1, w_2, z, v, y) = q_{\lambda_1}(w_1)q_{\lambda_2}(w_2)q_{\lambda_3}(z)q_{\lambda_4}(v) \prod_n q_{\lambda_{5, n}}(y_n).
\end{equation*}
The formula for the gradient of the ELBO w.r.t $\lambda_3$ is:
\begin{equation*}
  \nabla_{\lambda_3} \mathcal{L}(\lambda) = \mathbb{E}_{q(Z|X)}[\nabla_{\lambda_3} \log q_{\lambda_3}(z) (\log p_3(x, z) - \log q(z|\lambda_3))]
\end{equation*}
In this formula, as defined in the paper by Ranganath we have that $p_3(x,z)$ is the terms in the joint that depend
on those variables. Meaning that 
\begin{equation*}
  p_3(x, z) = p(x|z,y_n)p(z|v)
\end{equation*}
This gives the final expression for the Rao-Blackwellized partial gradient of the ELBO w.r.t $\lambda_3$:
\begin{equation*}
  \nabla_{\lambda_3} \mathcal{L}(\lambda) = \mathbb{E}_{q(Z|X)}[\nabla_{\lambda_3} \log q_{\lambda_3}(z) (\log p(x|z,y_n) + \log p(z|v) - \log q_{\lambda_3}(z))]
\end{equation*}
\subsection*{Practice/implementation - 1.C.3}
\subsubsection*{Question 2.2.13}
To implement BBVI with Control variates, but without Rao-Blackwellization, we reuse the code from 1.D.5 and add control variates.
The implementation can be found in the notebook BBVI.ipynb. The results were as follows:
\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{Plots/BBVI_Normal_Fit_CV_100_samples.png}
  \caption{ELBO over iterations for dataset with 100 samples using BBVI with control variates.}
\end{figure}

\subsection*{Practice/implementation - 1.C.4}
\subsubsection*{Question 2.2.14}
The gamma distribution pdf is defined as
\begin{equation*}
  p(x|\alpha, \beta) = \frac{x^{\alpha-1}}{\Gamma(\alpha)\beta^\alpha}\text{exp}(- x/\beta) = \text{exp}((\alpha-1)log(x) - \frac{x}{\beta} - log(\Gamma(\alpha)) - \alpha log(\beta))
\end{equation*}
In, canonical exponential-family form we can use the following identifications:
\begin{itemize}
    \item $\boldsymbol{\eta}(\boldsymbol{\theta}) = [\alpha - 1, -1/\beta]$
    \item $h(x) = 1$
    \item $\boldsymbol{T}(x) = [\log x, x]$
    \item $A(\boldsymbol{\eta}) = \log \Gamma(\eta_1 + 1) + (\eta_1 + 1)\log(-1/\eta_2) = \log \Gamma(\eta_1 + 1) - (\eta_1 + 1)\log(-\eta_2)$
\end{itemize}
With this we can identify
\begin{equation*}
  \nabla_{\boldsymbol{\eta}} A(\boldsymbol{\eta}) = \begin{bmatrix}
  \psi(\eta_1 + 1) - \log(-\eta_2) \\
  -\frac{\eta_1 + 1}{\eta_2}
  \end{bmatrix}
\end{equation*}
\subsubsection*{Question 2.2.15}
Let $J$ be the Jacobian matrix of $\eta$ w.r.t $\theta = (\alpha, \beta)$.
\begin{equation*}
  J = \begin{bmatrix}
  \frac{\partial \eta_1}{\partial \alpha} & \frac{\partial \eta_1}{\partial \beta} \\
  \frac{\partial \eta_2}{\partial \alpha} & \frac{\partial \eta_2}{\partial \beta}
  \end{bmatrix} = \begin{bmatrix}
  1 & 0 \\
  0 & \frac{1}{\beta^2}
  \end{bmatrix}
\end{equation*}
Furthermore, we have that $F(\eta)$ is the Fisher information matrix w.r.t the natural parameters $\eta$ and $F(\theta) = J^T F(\eta) J$.
According to the Question description, we do not need to specify the explicit entries in $F$.
\\
\\
Using this, we have that the natural gradient is given by:
\begin{equation*}
  \tilde{\nabla}_{\theta} \mathcal{L}(\theta) = F(\alpha, \beta)^{-1} \nabla_{\theta} \mathcal{L}(\theta) = J^T F(\eta)^{-1} J \nabla_{\theta} \mathcal{L}(\theta)
\end{equation*}
This follows from the fact that $J^{-1}=J$.
\subsubsection*{Question 2.2.16}
Now we compute the fisher information matrix $F(\theta)$ explicitly.
We have that $F(\eta) = \nabla_\eta\nabla_\eta^TA(\eta)$.
Thus,
\begin{equation*}
  F(\eta) = \begin{bmatrix}
  \psi_1(\eta_1 + 1) & -\frac{1}{\eta_2} \\
  -\frac{1}{\eta_2} & \frac{\eta_1 + 1}{\eta_2^2}
  \end{bmatrix}
\end{equation*}
where $\psi_1$ is the trigamma function.
Now we can compute $F(\theta)$, and get the result:
\begin{equation*}
  F(\theta) = J^T F(\eta) J = \begin{bmatrix}
  1 & 0 \\
  0 & \frac{1}{\beta^2}
  \end{bmatrix}
  \begin{bmatrix}
    \psi_1(\alpha) & \beta \\
    \beta & \alpha \beta^2
  \end{bmatrix}
  \begin{bmatrix}
  1 & 0 \\
  0 & \frac{1}{\beta^2}
  \end{bmatrix}
  = \begin{bmatrix}
  \psi_1(\alpha) & \frac{1}{\beta} \\
  \frac{1}{\beta} & \frac{\alpha}{\beta^2}
  \end{bmatrix}
\end{equation*}

\subsubsection*{Question 2.2.17}
The average negative log likelihood is given by:
\begin{equation*}
  \mathcal{L}(\alpha, \beta) = - \frac{1}{N}\log p(x_{1:N}|\alpha, \beta) = -\frac{1}{N}\sum_{i=1}^N \log p(x_i|\alpha, \beta)
\end{equation*}
(All samples are i.i.d).
Since we have samples from a Gamma distribution we can rewrite this as
\begin{equation*}
  \mathcal{L}(\alpha, \beta) = -\frac{1}{N}\sum_{n=1}^N \left( (\alpha - 1)\log x_n - \beta x_n - \log \Gamma(\alpha) + \alpha \log \beta \right)
\end{equation*}
The gradient of the negative log likelihood is then given by:
\begin{equation*}
  \nabla_{\alpha, \beta} \mathcal{L}(\alpha, \beta) = \begin{bmatrix}
  -\frac{1}{N}\sum_{n=1}^N \left( \log x_n - \psi(\alpha) + \log \beta \right) \\
  -\frac{1}{N}\sum_{n=1}^N \left( -x_n + \frac{\alpha}{\beta} \right)
  \end{bmatrix}
\end{equation*}
\subsubsection*{Question 2.2.18}
This question is primarily solved in a python notebook. We sample 1000 data points from $Gamma(\alpha^* = 3.0, \beta^*=2.0)$.
Using this data, we implement both gradient descent and natural gradient descent to estimate $\alpha$ and $\beta$.
We initilize both methods with a poor guess of $(\alpha, \beta) = (0.5,8)$ and ensure that $\alpha, \beta$ stay positive during optimizaiton.
The code can be found in ngd\_vs\_gd\_1D\_gamma\_AD.ipynb notebook.
\subsubsection*{Question 2.2.19}
The results of running both gradient descent and natural gradient descent can be seen in the following plots.
\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{Plots/gamma_parameters_evolution.png}
  \caption{Gradient descent: Estimated $\alpha$ and $\beta$ over iterations.}
\end{figure}
\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{Plots/gamma_nll_evolution.png}
  \caption{Natural gradient descent: Estimated $\alpha$ and $\beta$ over iterations.}
\end{figure}
Based on the plots, we can see that natural gradient descent converges faster to the true parameters $(\alpha^*, \beta^*) = (3.0, 2.0)$ compared to standard gradient descent.
Furthermore, natural gradient descent does not reach a good approximation, after 150 epochs but is in need of more epochs.
This is because natural gradient descent takes into account the geometry of the parameter space, leading to more efficient updates and avoids getting 'stuck'.
\section*{B-level}
\subsection*{1.B.1}
\subsubsection*{Question 3.1.20}
To approximate and reparameterize the categorical distribution we can use the Gumbel-Softmax distribution as an approximation.
Let $g_i\sim Gumbel(0,1)$ for $i=1,...,K$, probabilities $\pi_1,...,\pi_K$ and temperature $\tau$. 
A sample from the Gumbel-Softmax distribution is then given by:
\begin{equation*}
  y_i = \frac{\exp((\log(\pi_i) + g_i)/\tau)}{\sum_{j=1}^K \exp((\log(\pi_j) + g_j)/\tau)} \quad \text{for } i=1,...,K
\end{equation*}
This is the approximation of a one-hot encoded sample from a categorical distribution with class probabilities $\pi_1,...,\pi_K$. 
As $\tau \rightarrow 0$, the Gumbel-Softmax distribution approaches the categorical distribution, and with $\tau \rightarrow \infty$, it approaches a uniform distribution. 
Note that the approximation is continuous and differentiable w.r.t the parameters $\pi_i$, allowing for gradient-based optimization.
\\
\\
For evaluation, we can use the argmax function to obtain a one-hot encoded sample from the Gumbel-Softmax distribution:
\begin{equation*}
  z = \text{one\_hot}(\text{argmax}_i(g_i + \log(\pi_i)))
\end{equation*}
The following code was implemented:
\begin{minted}[fontsize=\small, linenos]{python}
# Hint: approximate the Categorical distribution with the Gumbel-Softmax distribution
def categorical_reparametrize(a, N, temp=0.1, eps=1e-20):  
  # temp and eps are hyperparameters for Gumbel-Softmax

  dist = Gumbel(0,1)
  u = dist.sample((N, a.shape[0]))
  samples = F.softmax((torch.log(a + eps) + u) / temp, dim=1)
  
  return samples # make sure that your implementation allows the gradient to backpropagate

\end{minted}
and the resulting output plot is shown below:
\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{Plots/compare.png}
  \caption{Output samples from the Gumbel-Softmax reparameterization.}
\end{figure}
\subsection*{1.B.2}
\subsubsection*{Question 3.1.21}
Using the reparameterization trick we can express a sample $z$ from a parametric distribution $q(z)$
as a deterministic function of a random variable $\epsilon$, with some fixed distribution and the parameters $\phi$
of $q_\phi(z)$, ($z = t(\epsilon, \phi)$). The paper gives the example of $q_\phi$ being a diagonal gaussian, and for $\epsilon \sim N(0, \mathbb{I})$,
$z = \mu + \sigma \epsilon$ gives a sample from $q_\phi(z) = N(z|\mu, \sigma^2)$. Under such a parametrization of $z$
we can decompose the total derivative of the integrand of the estimator, w.r.t trainable parameters $\phi$ as:
\begin{align}
  \hat{\nabla}_{TD}(\epsilon, \phi) = \nabla_\phi\left[\log p(\boldsymbol{x|z})+ \log p(\boldsymbol{z})-\log q_\phi(\boldsymbol{z|x})\right]  = \\
  \nabla_{\boldsymbol{z}}\left[\log p(\boldsymbol{x|z}) - \log q_\phi(\boldsymbol{z|x})\right] \nabla_\phi t(\epsilon, \phi) - \nabla_\phi \log q_\phi(\boldsymbol{z|x})
\end{align}
The reparameterized gradient estimator thus consists of two terms, the first is the path derivative and the second is the score function component.
\subsubsection*{Question 3.1.22}
\begin{equation*}
  \mathbb{E}_{q_\phi(z|x)}[\nabla_\phi \log q_\phi(z|x)] = \int q_\phi(z|x) \nabla_\phi \log q_\phi(z|x) dz = \int \nabla_\phi q_\phi(z|x) dz = \nabla_\phi \int q_\phi(z|x) dz = \nabla_\phi 1 = 0
\end{equation*}
The expectation of the score function is zero because the integral of the probability density function over its entire support is equal to 1, and the gradient of a constant (1) w.r.t any parameter is zero.
\newline
\subsubsection*{Question 3.1.23}
The authors propose that we can remove the score function term from the gradient estimate by setting $\phi'$ to the stop gradient.
\subsubsection*{Question 3.1.24}
The authors of the paper bring up the concept that if the score function is positively correlated with the remaining terms in the total derivative estimator,
then the variance of the estimator can be reduced by subtracting the score function term. (The score function then acts as a control variate)
\subsubsection*{Question 3.1.25}
After extending the VAE implementation of 2E according to Algorithm 2, the following results were obtained
for the ELBO over epochs on the MNIST dataset:
\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{Plots/Stop_gradient_VAE.png}
  \caption{ELBO over epochs on MNIST using path derivative estimator.}
\end{figure}
We can see that both models preform similarly, with the path derivative estimator having a slight edge.
This is expected as the score function term has an expectation of zero, meaning that removing it does not introduce bias.
However, removing the score function term can reduce variance in the gradient estimates, leading to more stable and efficient training.
The code used for implementing this can be found in the notebook 2E-VAE-HT25.ipynb
\section*{A-level}
\subsection*{1.A.1 - Theory}
\subsubsection*{Question 4.1.26}
From the paper Denoising Diffusion Probabilistic Models we have that the left most side of equation $3$ is
\begin{equation*}
  \mathbb{E} \left[ -\log p_\theta(\mathbf{x}_0) \right] \le \mathbb{E}_q \left[ -\log \frac{p_\theta(\mathbf{x}_{0:T})}{q(\mathbf{x}_{1:T}|\mathbf{x}_0)} \right] := L
\end{equation*}
Where the left hand side is the expecation taken over the data distribution and the right hand side is the ELBO (expectation taken over whole mean field assumption $q$).
Following the derivation in the paper we can rewrite this as:
\begin{align*}
  L = \mathbb{E}_q \left[ -\log p(\mathbf{x}_T) - \sum_{t \ge 1} \log \frac{p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)}{q(\mathbf{x}_t|\mathbf{x}_{t-1})} \right]\\
   = \mathbb{E}_q \left[ -\log p(\mathbf{x}_T) - \sum_{t > 1} \log \frac{p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)}{q(\mathbf{x}_t|\mathbf{x}_{t-1}, \mathbf{x}_0)} - \log \frac{q(\mathbf{x}_t|\mathbf{x}_{t-1}, \mathbf{x}_0)}{q(\mathbf{x}_t|\mathbf{x}_{t-1})} \right]\\
   = \mathbb{E}_q \left[ -\log p(\mathbf{x}_T) - \sum_{t > 1} \log \frac{p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)}{q(\mathbf{x}_{t-1}|\mathbf{x}_{t}, \mathbf{x}_0)} \cdot \frac{q(\mathbf{x}_{t-1}|\mathbf{x}_0)}{q(\mathbf{x}_{t}| \mathbf{x}_0)} - \log \frac{p_\theta(\mathbf{x}_{0}|\mathbf{x}_1)}{q(\mathbf{x}_{1}|\mathbf{x}_0)}\right]\\
   = \mathbb{E}_q \left[ -\log \frac{p(\mathbf{x}_T)}{q(\mathbf{x}_T|\mathbf{x}_0)} - \sum_{t > 1}\log \frac{p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)}{q(\mathbf{x}_{t-1}|\mathbf{x}_{t}, \mathbf{x}_0)} - \log p_\theta(\mathbf{x}_0|\mathbf{x}_1)\right] \\
   = \mathbb{E}_q \left[ D_{KL}(q(\mathbf{x}_T|\mathbf{x}_0) || p(\mathbf{x}_T)) + \sum_{t > 1} D_{KL}(q(\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_0) || p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)) - \log p_\theta(\mathbf{x}_0|\mathbf{x}_1) \right]
\end{align*}
The expectations are all taken over the full distribution $q(\mathbf{x}_{0:T})$.
The final step, to arrive at the final equation comes from separating the terms in the expectations.
\begin{align*}
  L = \mathbb{E}_q \left[ D_{KL}(q(\mathbf{x}_T|\mathbf{x}_0) || p(\mathbf{x}_T))\right] + \sum_{t > 1} \mathbb{E}_q \left[ D_{KL}(q(\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_0) || p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t))\right] - \mathbb{E}_q \left[ \log p_\theta(\mathbf{x}_0|\mathbf{x}_1) \right]
\end{align*}
In this final expression we can identify the three terms as $L_T$, $L_{t-1}$ and $L_0$ respectively.
\begin{equation*}
  L_T = D_{KL}(q(\mathbf{x}_T|\mathbf{x}_0) || p(\mathbf{x}_T)) \rightarrow (1) = \mathbb{E}_q \left[ L_T \right]
\end{equation*}
\begin{equation*}
  L_{t-1} = D_{KL}(q(\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_0) || p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)) \rightarrow (2) = \mathbb{E}_q \left[ L_{t-1} \right]
\end{equation*}
\begin{equation*}
  L_0 = \log p_\theta(\mathbf{x}_0|\mathbf{x}_1) \rightarrow (3) = \mathbb{E}_q \left[ L_0 \right]
\end{equation*}
By inspecting the final expectations we find what distributions the expectations are taken over.
We have that
\begin{equation*}
  (1) = \mathbb{E}_q \left[ L_T \right] = \mathbb{E}_{q(\mathbf{x}_0)} \left[ D_{KL}(q(\mathbf{x}_T|\mathbf{x}_0) || p(\mathbf{x}_T))\right]
\end{equation*}
Meaning that we only take the expectation over $\mathbf{x}_0$. This is true since the Kullback-leibler divergence integrates out $\mathbf{x}_T$.
\begin{equation*}
  (2) = \mathbb{E}_q \left[ L_{t-1} \right] = \mathbb{E}_{q(\mathbf{x}_t, \mathbf{x}_0)} \left[ D_{KL}(q(\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_0) || p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t))\right]
\end{equation*}
Showing that we take the expectation over $\mathbf{x}_t$ and $\mathbf{x}_0$. This is once again true since the Kullback-leibler divergence integrates out $\mathbf{x}_{t-1}$.
Finally, we have that
\begin{equation*}
  (3) = \mathbb{E}_q \left[ L_0 \right] = \mathbb{E}_{q(\mathbf{x}_1, \mathbf{x}_0)} \left[ \log p_\theta(\mathbf{x}_0|\mathbf{x}_1) \right]
\end{equation*}
Taking the expectation over $\mathbf{x}_1$ and $\mathbf{x}_0$. Thus we have specified the random variables of the expectations.
\subsection*{1.A.2 - Importance Weighted Autoencoder}
\subsubsection*{Question 4.2.27}
For this question we extend the VAE implementation from 2E to the IWAE model.
To implement this, the loss function is modified to use the IWELBO with $K$ samples.
The modified loss function is as follows:
\begin{equation*}
  \mathcal{L}_K = \mathbb{E}_{Z_{1}, \ldots, Z_{K}}\left[\ \log\left( \frac{1}{K} \sum_{k=1}^{K} \frac{p_\theta(X, Z_k)}{q_\phi(Z_k|X)}\right)\right]
\end{equation*}
Let $W_k = \frac{p_\theta(X, Z_k)}{q_\phi(Z_k|X)}$. The loss function can then be rewritten as:
\begin{equation*}
  \mathcal{L}_K = \mathbb{E}_{Z_{1}, \ldots, Z_{K}}\left[\ \log\left( \frac{1}{K} \sum_{k=1}^{K} W_k\right)\right]
\end{equation*}
In practice, we approximate the expectation using Monte Carlo sampling.
\\
\\
The sampling also changes, and we now sample $K$ latent variables.
This is done by modifying the reparametrization function to sample $K$ times from the latent distribution.
The following code snippets show the modified parts of the VAE implementation:
The reparametrization function:
\begin{minted}[fontsize=\small, linenos]{python}
def reparameterization(self, mean, std, K):
    # set z = mean + std * epsilon, where epsilon ~ N(0, I)
    batch_size, latent_dim = mean.size()
    eps = torch.randn(batch_size, K, latent_dim).to(device)  # shape: (batch_size, K, latent_dim)
    z = mean.unsqueeze(1) + std.unsqueeze(1) * eps  # shape: (batch_size, K, latent_dim)
    return z
\end{minted}
The loss function:
\begin{minted}[fontsize=\small, linenos]{python}
def loss_IWAE(x, theta, mean, log_var, z):
    # theta: (batch, K, x_dim), z: (batch, K, latent_dim)
    #Expanded x to match theta's shape
    x_expanded = x.unsqueeze(1).expand_as(theta)
    #Calculate log p(x|z)
    log_px_z= -nn.functional.binary_cross_entropy(theta, x_expanded,
      reduction='none').sum(dim=2)

    # Parameters for calculating log probabilities
    log2pi = float(np.log(2 * np.pi))
    #Expand mean and log_var to match z's shape
    mean_exp = mean.unsqueeze(1)
    log_var_exp = log_var.unsqueeze(1)

    #prior p(z) ~ N(0, I)
    log_pz = -0.5 * (z.pow(2) + log2pi).sum(dim=2)

    #posterior q(z|x) ~ N(mean, var)
    log_qz_x = -0.5 * ((((z - mean_exp) ** 2) / log_var_exp.exp()) +
     log_var_exp + log2pi).sum(dim=2)

    log_w = log_px_z + log_pz - log_qz_x
    
    # Loss for each batch element
    log_w_mean = torch.logsumexp(log_w, dim=1) - np.log(log_w.size(1))

    #mean over batch:
    loss = -log_w_mean.mean()
    return loss
\end{minted}
\subsubsection*{Question 4.2.28}
Following the experimental setup from 2E (section 5.1) we use the 1 stochastic layer model on MNIST.
We train the regular VAE model, together with the IWAE model with $K = 1$, $K=5$ and $K=50$. 
For feasible runtime, $\text{numer of epochs} = 20$ and learning rate $= 1e-3$.
The results for the ELBO over epochs are shown in the following plot:
%no plot yet, use placeholder
\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{Plots/IWAE_different_K.png}
  \caption{ELBO over epochs on MNIST using IWAE with different K values.}
\end{figure}
We can clearly see that increasing $K$ leads to better ELBO values.
This is inline with the theory, since the IWELBO is a tighter bound.
For the model architecure, I left the encoder and decoder the same as in 2E.
\begin{minted}{python}
class Encoder(nn.Module):
  # encoder outputs the parameters of variational distribution "q"
  def __init__(self, input_dim, hidden_dim, latent_dim):
      super(Encoder, self).__init__()

      self.FC_enc1 = nn.Linear(input_dim, hidden_dim) # FC = fully connected layer
      self.FC_enc2 = nn.Linear(hidden_dim, hidden_dim)
      self.FC_mean = nn.Linear(hidden_dim, latent_dim)
      self.FC_std  = nn.Linear (hidden_dim, latent_dim)

      self.LeakyReLU = nn.LeakyReLU(0.2)

      self.training = True

  def forward(self, x):
      h_1     = self.LeakyReLU(self.FC_enc1(x))
      h_2     = self.LeakyReLU(self.FC_enc2(h_1))
      mu    = self.FC_mean(h_2)  # mean / location
      log_var = self.FC_std(h_2)   # log variance

      return mu, log_var

class Decoder(nn.Module):
    # decoder generates the success parameter of each pixel
    def __init__(self, latent_dim, hidden_dim, output_dim):
        super(Decoder, self).__init__()
        self.FC_dec1   = nn.Linear(latent_dim, hidden_dim)
        self.FC_dec2   = nn.Linear(hidden_dim, hidden_dim)
        self.FC_output = nn.Linear(hidden_dim, output_dim)

        self.LeakyReLU = nn.LeakyReLU(0.2)

    def forward(self, z):
        h_out_1  = self.LeakyReLU(self.FC_dec1(z))
        h_out_2  = self.LeakyReLU(self.FC_dec2(h_out_1))

        theta = torch.sigmoid(self.FC_output(h_out_2))
        return theta
\end{minted}
Then, I changed the model containing the reparameterization to the following:
\begin{minted}{python}
class Model(nn.Module):
    def __init__(self, Encoder, Decoder):
        super(Model, self).__init__()
        self.Encoder = Encoder
        self.Decoder = Decoder

    def reparameterization(self, mean, std):
        # set z = mean + std * epsilon, where epsilon ~ N(0, I)
        z = mean + std * torch.randn_like(std)
        return z

    def forward(self, x):
        mean, log_var = self.Encoder(x)
        std = torch.exp(0.5 * log_var)
        z = self.reparameterization(mean, std)
        theta = self.Decoder(z)
        return theta, mean, log_var, z
    
class Model_IWAE(nn.Module):
    def __init__(self, Encoder, Decoder):
        super(Model_IWAE, self).__init__()
        self.Encoder = Encoder
        self.Decoder = Decoder

    #Sample K times
    def reparameterization(self, mean, std, K):
        # set z = mean + std * epsilon, where epsilon ~ N(0, I)
        batch_size, latent_dim = mean.size()
        eps = torch.randn(batch_size, K, latent_dim).to(device)  # shape: (batch_size, K, latent_dim)
        z = mean.unsqueeze(1) + std.unsqueeze(1) * eps  # shape: (batch_size, K, latent_dim)
        return z

    def forward(self, x, K):
        mean, log_var = self.Encoder(x)
        std = torch.exp(0.5 * log_var)
        z = self.reparameterization(mean, std, K)  # shape: (batch_size, K, latent_dim)
        theta = self.Decoder(z.view(-1, z.size(-1)))  # reshape z to (batch_size * K, latent_dim)
        theta = theta.view(x.size(0), K, -1)  # reshape theta to (batch_size, K, x_dim)
        return theta, mean, log_var, z
\end{minted}
\section*{Appendix}
(For a better overview of the code, please refer to the respective notebooks and scripts)
\subsection*{Code - BBVI.ipynb}
\inputminted{python}{Code/BBVI.py}
\subsection*{Code - 2E-VAE-HT25.ipynb}
\inputminted{python}{Code/2E-VAE-HT25.py}
\subsection*{Code - ngd\_vs\_gd\_1D\_gamma\_AD.ipynb}
\inputminted{python}{Code/ngd_vs_gd_1D_gamma_AD.py}
\subsection*{Code - 1AD-B1.py}
\inputminted{python}{Code/1AD-B1.py}
\end{document}
